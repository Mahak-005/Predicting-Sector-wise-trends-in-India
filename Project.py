# -*- coding: utf-8 -*-
"""CAPSTONES.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g3V-xP6Up62Vrdgfc6GgKOwerrsuVbxD
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib as mpl
import plotly.express as ex
import plotly.graph_objs as go
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
from sklearn.decomposition import PCA
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import adfuller
from statsmodels.tools.eval_measures import rmse, aic
import matplotlib.pyplot as plt
import numpy as np
import os
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns

df= pd.read_csv("/content/CAPSTONES DS.csv")
df.head(10)

df.shape

df.describe()

df.dtypes

df.columns

columns_to_convert = ['Value_added_nominal', 'Value_added_real',
       'Employment', 'Labor_productivity_real', 'Labor_productivity_PPP']

for column in columns_to_convert:
    df[column] = df[column].fillna(0).astype(int)

df.dtypes

df.head(5)

sns.set_palette(sns.color_palette("pastel"))

# Create a heatmap for missing values
plt.figure(figsize=(6, 6))
sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='Pastel2')
plt.title('Missing Values Heatmap ')
plt.show()

df2 = df.drop(columns=['sector'])
corr_matrix = df2.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='Pastel1')
plt.title('Correlation Plot')
plt.show()

# Function to detect outliers using IQR
def find_outliers(column):
    q1 = column.quantile(0.25)
    q3 = column.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    return column[(column < lower_bound) | (column > upper_bound)]

# Detect outliers for each numerical column
outliers = {}
for column in df.select_dtypes(include='number'):
    outliers[column] = find_outliers(df[column])

# Print outliers for each column
for column, values in outliers.items():
    if not values.empty:
        print(f"Outliers in '{column}':")
        print(values)

import pandas as pd

# Assuming df is your DataFrame containing the data
# Remove non-numeric columns before outlier detection
numeric_df = df.select_dtypes(include=['float64', 'int64'])

# Define a function to detect outliers using z-score
def detect_outliers_zscore(data, threshold=3):
    z_scores = (data - data.mean()) / data.std()
    return (z_scores > threshold) | (z_scores < -threshold)

# Define a function to detect outliers using IQR
def detect_outliers_iqr(data, threshold=1.5):
    q1 = data.quantile(0.25)
    q3 = data.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - threshold * iqr
    upper_bound = q3 + threshold * iqr
    return (data < lower_bound) | (data > upper_bound)

# Initialize a dictionary to store the count of outliers for each variable
outlier_counts = {}

# Iterate over each column in the numeric dataframe to count outliers
for column in numeric_df.columns:
    # Count outliers using z-score method
    zscore_outliers_count = detect_outliers_zscore(numeric_df[column]).sum()

    # Count outliers using IQR method
    iqr_outliers_count = detect_outliers_iqr(numeric_df[column]).sum()

    # Store the counts in the dictionary
    outlier_counts[column] = {
        'zscore_outliers_count': zscore_outliers_count,
        'iqr_outliers_count': iqr_outliers_count
    }

# Display the count of outliers for each variable
for column, values in outlier_counts.items():
    print(f"Outlier counts in {column}:")
    print("Z-Score Outliers Count:", values['zscore_outliers_count'])
    print("IQR Outliers Count:", values['iqr_outliers_count'])
    print()

plt.figure(figsize=(12, 8))
for i, column in enumerate(df.columns[2:], start=1):
    plt.subplot(3, 3, i)
    plt.hist(df[column], color=plt.cm.tab10(i))
    plt.title(column)


plt.tight_layout()
plt.show()

sector = df['sector'].value_counts()
print("sector:")
print(sector)

category_mapping = {
    'Total': 1,
    '1.Agriculture': 2,
    '2.Mining': 3,
    '3.Manufacturing': 4,
    '4.Utilities': 5,
    '5.Construction': 6,
    '6.Trade services': 7,
    '7.Transport services': 8,
    '8.Finance amd business services': 9,
    '9.Other services': 10
}

# Replace categories with numbers
df['sector'] = df['sector'].replace(category_mapping)

print("Updated Sectors:")
print(df['sector'].value_counts())

df.head(5)

"""EXPLORATORY DATA ANALYSIS"""

##EDA
sns.set_palette(sns.color_palette("pastel"))

# Create box plots for each column in df
plt.figure(figsize=(12, 6))
sns.boxplot(data=df)
plt.title('Boxplot of Each Column')
plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility
plt.show()

sns.set_palette("deep")

# Count the number of observations for each plant category
sector_counts = df['sector'].value_counts()

# Create a bar graph
plt.figure(figsize=(12, 6))
sns.barplot(x=sector_counts.index, y=sector_counts.values)
plt.xlabel('sector')
plt.ylabel('Number of Observations')
plt.title('Number of Observations for Each Sector')
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

sns.pairplot(df2)
plt.title('Pair Plot')
plt.show()

# Plot histograms for each sector with Employment on y-axis
df_hist = df[['sector', 'Employment']]

# Create subplots for each sector
fig, axs = plt.subplots(figsize=(10, 6))
for sector, data in df_hist.groupby('sector'):
    axs.hist(data['Employment'], bins=20, alpha=0.5, label=sector)

plt.title('Frequency of Employment Values for Each Sector')
plt.xlabel('Employment')
plt.ylabel('Frequency')
plt.legend(title='Sector', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.show()

from statsmodels.tsa.stattools import adfuller

# Function to perform ADF test
def adfuller_test(series, signif=0.05, name=''):
    r = adfuller(series.dropna(), autolag='AIC')
    output = {'test_statistic': round(r[0], 4), 'pvalue': round(r[1], 4), 'n_lags': round(r[2], 4), 'n_obs': r[3]}
    p_value = output['pvalue']
    def adjust(val, length=6): return str(val).ljust(length)

    # Print the results
    print(f'    Augmented Dickey-Fuller Test on "{name}"', "\n   ", '-'*47)
    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')
    print(f' Significance Level    = {signif}')
    print(f' Test Statistic        = {output["test_statistic"]}')
    print(f' No. Lags Chosen       = {output["n_lags"]}')

    for key, val in r[4].items():
        print(f' Critical value {adjust(key)} = {round(val, 3)}')

    if p_value <= signif:
        print(f" => P-Value = {p_value}. Rejecting Null Hypothesis.")
        print(f" => Series is Stationary.")
    else:
        print(f" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.")
        print(f" => Series is Non-Stationary.")

# ADF Test on each column
for name, column in df.items():  # Use df.items() instead of df.iteritems()
    adfuller_test(column, name=column.name)
    print('\n')

"""STANDARIZATION"""

# Split data into training and testing sets
X = df.drop('sector', axis=1)
y = df['sector']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

pca=PCA(n_components=2)
PC=pca.fit_transform(X)
principalDF=pd.DataFrame(data=PC,columns=['pc1','pc2'])
df = pd.concat([principalDF, df.reset_index(drop=True)['sector']], axis = 1)
df

"""PREDICTIVE MODEL

LINEAR REGRESSION MODEL
"""

#LINEAR REGRESSION
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X = df[['Employment']]  # Independent variable
y = df['Employment']  # Dependent variable

# Assuming X_train, X_test, y_train, y_test are your training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)
y_pred_linear = linear_reg.predict(X_test)
rmse_linear = mean_squared_error(y_test, y_pred_linear, squared=False)
print("Linear Regression RMSE:", rmse_linear)

#Accuracy
from sklearn.metrics import r2_score
r2_linear = r2_score(y_test, y_pred_linear)
print("Linear Regression R-squared:", r2_linear)

"""RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

"""FORECASTING"""

df.plot(subplots=True)

!pip install pmdarima

import pmdarima as pm

model = pm.auto_arima(df['Labor_productivity_PPP'],
                        m=12, seasonal=True,
                      start_p=0, start_q=0, max_order=4, test='adf',error_action='ignore',
                           suppress_warnings=True,
                      stepwise=True, trace=True)

model.summary()

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error
# Preprocess your data if necessary
# For SARIMA, ensure that your dataset is in a univariate time series format

# Split your data into training and testing sets
train_size = int(len(df) * 0.8)
train_data, test_data = df.iloc[:train_size]['Value_added_real'], df.iloc[train_size:]['Value_added_real']

# Train the SARIMA model
order = (1, 1, 1)  # Example order of ARIMA parameters (p, d, q)
seasonal_order = (1, 1, 1, 12)  # Example seasonal order (P, D, Q, S)
model = SARIMAX(train_data, order=order, seasonal_order=seasonal_order)
model_fit = model.fit()

# Make predictions
predictions = model_fit.forecast(steps=len(test_data))

# Evaluate the model
mse = mean_squared_error(test_data, predictions)
print("Mean Squared Error:", mse)

# Plot the predictions
plt.plot(test_data.index, test_data.values, label='Actual')
plt.plot(test_data.index, predictions, label='Predicted')
plt.xlabel('Time')
plt.ylabel('Value_added_real')
plt.legend()
plt.show()































bhhbhj





jb

hhbh









































































